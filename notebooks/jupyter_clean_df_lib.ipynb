{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T23:12:37.834739Z",
     "iopub.status.busy": "2025-02-23T23:12:37.834346Z",
     "iopub.status.idle": "2025-02-23T23:12:42.126873Z",
     "shell.execute_reply": "2025-02-23T23:12:42.125421Z",
     "shell.execute_reply.started": "2025-02-23T23:12:37.834709Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.10/dist-packages (3.12.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install rapidfuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-23T23:12:42.128829Z",
     "iopub.status.busy": "2025-02-23T23:12:42.128536Z",
     "iopub.status.idle": "2025-02-23T23:12:42.134410Z",
     "shell.execute_reply": "2025-02-23T23:12:42.133082Z",
     "shell.execute_reply.started": "2025-02-23T23:12:42.128803Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'chardet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mchardet\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrapidfuzz\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m process, fuzz\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m winsorize\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'chardet'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import chardet\n",
    "from rapidfuzz import process, fuzz\n",
    "from scipy.stats.mstats import winsorize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ## 1. Carga de datos: detect_load_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T23:12:42.136886Z",
     "iopub.status.busy": "2025-02-23T23:12:42.136514Z",
     "iopub.status.idle": "2025-02-23T23:12:42.157199Z",
     "shell.execute_reply": "2025-02-23T23:12:42.156150Z",
     "shell.execute_reply.started": "2025-02-23T23:12:42.136844Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def detect_load_data(csv_path, encoding=None, char_num=10000):\n",
    "    \"\"\"Function to detect the encoding of the CSV file and load it.\n",
    "    \n",
    "    Parameters:\n",
    "    - csv_path (str): Path to the CSV file.\n",
    "    - encoding (str, optional): Encoding to use. If None, it will be detected automatically.\n",
    "    - char_num (int, optional): Number of characters to read for encoding detection.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame or None: Loaded dataframe, or None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if encoding is None:\n",
    "            with open(csv_path, 'rb') as rawdata:\n",
    "                result = chardet.detect(rawdata.read(char_num))\n",
    "            encoding = result.get(\"encoding\", \"utf-8\")\n",
    "            print(f\"Detected encoding: {encoding}\")\n",
    "        \n",
    "        df = pd.read_csv(csv_path, encoding=encoding)\n",
    "        return df\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: The file was not found.\")\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(\"Error: The file is empty.\")\n",
    "    except pd.errors.ParserError:\n",
    "        print(\"Error: The file could not be parsed.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "    \n",
    "    return None  # Return None if an error occurs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploración inicial: first_view_data\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T23:12:42.159323Z",
     "iopub.status.busy": "2025-02-23T23:12:42.158913Z",
     "iopub.status.idle": "2025-02-23T23:12:42.181593Z",
     "shell.execute_reply": "2025-02-23T23:12:42.180203Z",
     "shell.execute_reply.started": "2025-02-23T23:12:42.159285Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def first_view_data(df, heat=True, colors=[\"#000099\", \"#ffff00\"], title=\"Missing Values\"):\n",
    "    \"\"\"Performs an initial exploration of the dataframe.\n",
    "    \n",
    "    This function normalizes column names and string values, displays \n",
    "    key information, and visualizes missing data.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The dataframe to analyze.\n",
    "    - heat (bool, optional): If True, displays a heatmap of missing values. \n",
    "      If False, shows a bar plot of missing values.\n",
    "    - colors (list, optional): Color palette for the heatmap.\n",
    "    - title (str, optional): Title for the visualization.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The cleaned dataframe.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Normalize string columns\n",
    "    df_obj = df.select_dtypes(include=[\"object\"])\n",
    "    df[df_obj.columns] = df_obj.applymap(lambda s: s.lower().strip() if isinstance(s, str) else s)\n",
    "\n",
    "    # Normalize column names\n",
    "    df.columns = df.columns.str.lower().str.strip()\n",
    "\n",
    "    # Display basic info\n",
    "    display(df.head())\n",
    "    display(df.tail())\n",
    "    df.info()\n",
    "\n",
    "    # Visualize missing values\n",
    "    if heat:\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        sns.heatmap(df.isnull(), cbar=False, cmap=sns.color_palette(colors))\n",
    "        plt.xticks(rotation=70)\n",
    "        plt.title(title)  # Añadir título\n",
    "        plt.show()\n",
    "    else:\n",
    "        df.isnull().sum().plot.bar(figsize=(10, 4), alpha=0.75, rot=70, color=\"red\", fontsize=12)\n",
    "        plt.title(title)  # Añadir título\n",
    "        plt.show()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Limpieza básica: remove_invalid_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T23:12:42.183029Z",
     "iopub.status.busy": "2025-02-23T23:12:42.182733Z",
     "iopub.status.idle": "2025-02-23T23:12:42.208600Z",
     "shell.execute_reply": "2025-02-23T23:12:42.207444Z",
     "shell.execute_reply.started": "2025-02-23T23:12:42.183004Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def remove_invalid_data(df, nan_col_per=51, nan_row_per=51, object_threshold=3):\n",
    "    \"\"\"Function that drops duplicated rows, rows and columns with NaN values above the threshold, \n",
    "    and removes categorical columns with too few unique values.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame.\n",
    "    - nan_col_per: Percentage threshold for columns with NaN values to be dropped (default 51%).\n",
    "    - nan_row_per: Percentage threshold for rows with NaN values to be dropped (default 51%).\n",
    "    - object_threshold: Threshold for number of unique values to drop categorical columns with too few values (default 3).\n",
    "    \n",
    "    Returns:\n",
    "    - df: pandas DataFrame after cleaning.\n",
    "    \"\"\"\n",
    "    # Drop duplicate rows\n",
    "    df = df.drop_duplicates()\n",
    "    # Drop duplicate columns by transposing the DataFrame, dropping duplicates, and transposing back\n",
    "    df = df.T.drop_duplicates().T\n",
    "    \n",
    "    # Drop rows with NaN values above the threshold percentage\n",
    "    # Calculate the minimum number of non-NaN values required to keep a row\n",
    "    thresh_col_drop = round(len(df) * (1 - nan_row_per / 100), 0)\n",
    "    # Drop rows that don't meet the threshold\n",
    "    df = df.dropna(thresh=(thresh_col_drop), axis=0)\n",
    "    \n",
    "    # Drop columns with NaN values above the threshold percentage\n",
    "    # Calculate the minimum number of non-NaN values required to keep a column\n",
    "    thresh_row_drop = round(len(df.columns) * (1 - nan_col_per / 100), 0)\n",
    "    # Drop columns that don't meet the threshold\n",
    "    df = df.dropna(thresh=(thresh_row_drop), axis=1)\n",
    "    \n",
    "    # Drop categorical (object) columns with too few unique values\n",
    "    # Select columns of type 'object' and keep only those with more unique values than the threshold\n",
    "    df = df.loc[:, (df.select_dtypes(include=['object']).nunique() > object_threshold)]\n",
    "    \n",
    "    # Calculate and display the percentage of NaNs in each column\n",
    "    null_percent = [round(100 * df[column].isnull().sum() / df.shape[0], 1) \n",
    "                    for column in df.columns]\n",
    "    # Display the NaN percentages in a DataFrame\n",
    "    display(pd.DataFrame(np.array(null_percent), \n",
    "                         index=df.columns, columns=[\"NaN_%\"]).T)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 4. Conversión de tipos: search_transf_cat, search_transf_num, search_transf_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T23:12:42.209855Z",
     "iopub.status.busy": "2025-02-23T23:12:42.209586Z",
     "iopub.status.idle": "2025-02-23T23:12:42.233571Z",
     "shell.execute_reply": "2025-02-23T23:12:42.232424Z",
     "shell.execute_reply.started": "2025-02-23T23:12:42.209833Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def search_transf_num(df):\n",
    "    \"\"\"This function searches for non-numeric columns in the dataframe\n",
    "    and attempts to convert them to numeric. It returns the dataframe with\n",
    "    the correct column types and prints out the columns that were changed.\n",
    "    \"\"\"\n",
    "    \n",
    "    to_num = []  # List to store the names of columns that were successfully converted to numeric\n",
    "    \n",
    "    # Select columns that are not numeric (excluding datetime and timedelta types)\n",
    "    df_non_numeric = df.select_dtypes(exclude=[np.number, 'datetime', 'timedelta'])\n",
    "    \n",
    "    # Iterate over each non-numeric column\n",
    "    for column in df_non_numeric.columns:\n",
    "        # Clean non-numeric characters (e.g., commas, currency symbols) that could interfere with conversion\n",
    "        df[column] = df[column].replace(r\"[^\\d.-]\", \"\", regex=True)\n",
    "        \n",
    "        # Attempt to convert the column to a numeric type\n",
    "        try:\n",
    "            df[column] = pd.to_numeric(df[column], errors='coerce')  # 'coerce' will convert invalid values to NaN\n",
    "            # If the conversion is successful, add the column name to the list\n",
    "            to_num.append(column)\n",
    "        except Exception as e:\n",
    "            # If an error occurs during conversion, skip to the next column\n",
    "            continue\n",
    "    \n",
    "    # Print the results\n",
    "    if to_num:\n",
    "        print(f\"The column/s changed to numeric: {', '.join(to_num)}\")\n",
    "    else:\n",
    "        print(\"No columns were converted to numeric.\")\n",
    "    \n",
    "    return df  # Return the dataframe with updated column types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T23:12:42.234943Z",
     "iopub.status.busy": "2025-02-23T23:12:42.234655Z",
     "iopub.status.idle": "2025-02-23T23:12:42.255966Z",
     "shell.execute_reply": "2025-02-23T23:12:42.254709Z",
     "shell.execute_reply.started": "2025-02-23T23:12:42.234909Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def search_transf_cat(df, percent=5):\n",
    "    \"\"\"This function searches for columns with categorical data types based on \n",
    "    the number of unique values in each column compared to a percentage threshold.\n",
    "    It then converts them to 'category' type and prints out the columns that were changed.\n",
    "    \"\"\"\n",
    "    # Calculate the threshold for the number of unique values\n",
    "    threshold = len(df) * (percent / 100)\n",
    "    \n",
    "    # Filter non-numeric columns\n",
    "    df_non_numeric = df.select_dtypes(exclude=[np.number, 'datetime', 'timedelta'])\n",
    "    \n",
    "    prob_cat = [] \n",
    "    \n",
    "    for column in df_non_numeric.columns:\n",
    "        # Check if the number of unique values is below the threshold\n",
    "        if len(df[column].value_counts()) < threshold:\n",
    "            # Convert to 'category' type if condition met\n",
    "            df[column] = df[column].astype(\"category\")\n",
    "            prob_cat.append(column)\n",
    "    \n",
    "    print(f\"The column/s changed to categoric is/are {prob_cat}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T23:12:42.259038Z",
     "iopub.status.busy": "2025-02-23T23:12:42.258729Z",
     "iopub.status.idle": "2025-02-23T23:12:42.276866Z",
     "shell.execute_reply": "2025-02-23T23:12:42.275800Z",
     "shell.execute_reply.started": "2025-02-23T23:12:42.259011Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def search_transf_date(df, date_columns=[], new_columns=False, drop_old=False, regex_pattern=None, custom_formats=None):\n",
    "    \"\"\"\n",
    "    Converts specified columns to datetime and optionally creates new columns for year, month, and day.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame to process.\n",
    "    - date_columns: List of columns to convert to datetime.\n",
    "    - new_columns: If True, creates new columns for year, month, and day.\n",
    "    - drop_old: If True, drops the original date columns.\n",
    "    - regex_pattern: Custom regex pattern to clean data (default is None).\n",
    "    - custom_formats: List of custom datetime formats to try (default is None).\n",
    "    \"\"\"\n",
    "    if regex_pattern is None:\n",
    "        regex_pattern = r\"[^\\w\\s\\d\\-\\:/\\.]\"  # Default regex to clean unwanted characters\n",
    "\n",
    "    for col in date_columns:\n",
    "        # Clean the column using the provided regex pattern\n",
    "        df[col] = df[col].str.replace(regex_pattern, \"\", regex=True)\n",
    "        \n",
    "        # Convert to datetime with error handling\n",
    "        if custom_formats:\n",
    "            for fmt in custom_formats:\n",
    "                try:\n",
    "                    df[col] = pd.to_datetime(df[col], format=fmt, errors=\"raise\")\n",
    "                    break\n",
    "                except ValueError:\n",
    "                    continue\n",
    "            else:\n",
    "                df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "        else:\n",
    "            df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "        \n",
    "        # Ensure the column is of type datetime64[D]\n",
    "        df[col] = df[col].astype(\"datetime64[D]\", errors=\"ignore\")\n",
    "        \n",
    "        if new_columns:\n",
    "            # Create new columns for year, month, and day based on the cleaned date column\n",
    "            df[f\"{col}_year\"] = df[col].dt.year\n",
    "            df[f\"{col}_month\"] = df[col].dt.month\n",
    "            df[f\"{col}_day\"] = df[col].dt.day\n",
    "            \n",
    "            if drop_old:\n",
    "                df = df.drop([col], axis=1)\n",
    "                \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T23:12:42.278831Z",
     "iopub.status.busy": "2025-02-23T23:12:42.278505Z",
     "iopub.status.idle": "2025-02-23T23:12:42.294400Z",
     "shell.execute_reply": "2025-02-23T23:12:42.293015Z",
     "shell.execute_reply.started": "2025-02-23T23:12:42.278802Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def numeric_fill_nan(df, columns=[], fill=\"mean\"):\n",
    "    \"\"\"This function fills the NaN values in specified numeric columns with a specified method\n",
    "    such as mean, median, or a custom value.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to process\n",
    "        columns: List of columns to fill NaN values in (if empty, all numeric columns are processed)\n",
    "        fill: Method to fill NaN values. Options are 'mean', 'median', or a custom value. Default is 'mean'.\n",
    "    \n",
    "    Returns:\n",
    "        Updated DataFrame with NaN values filled.\n",
    "    \"\"\"\n",
    "    # If no columns are specified, fill NaNs in all numeric columns\n",
    "    if not columns:\n",
    "        columns = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    # Validate that all specified columns exist in the DataFrame\n",
    "    columns = [col for col in columns if col in df.columns]\n",
    "    \n",
    "    # Loop over the selected columns\n",
    "    for column in columns:\n",
    "        if df[column].dtype in [np.number, 'float64', 'int64']:  # Ensure the column is numeric\n",
    "            if fill == \"median\":\n",
    "                df[column].fillna(df[column].median(), inplace=True)\n",
    "            elif fill == \"mean\":\n",
    "                df[column].fillna(df[column].mean(), inplace=True)\n",
    "            else:\n",
    "                df[column].fillna(fill, inplace=True)\n",
    "        else:\n",
    "            print(f\"Warning: Column '{column}' is not numeric and will be skipped.\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T23:12:42.295830Z",
     "iopub.status.busy": "2025-02-23T23:12:42.295523Z",
     "iopub.status.idle": "2025-02-23T23:12:42.317551Z",
     "shell.execute_reply": "2025-02-23T23:12:42.316376Z",
     "shell.execute_reply.started": "2025-02-23T23:12:42.295802Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def categoric_fill_nan(df, columns=None):\n",
    "    \"\"\"Fills the categorical columns with the mode value (most frequent) for each column in the provided list.\"\"\"\n",
    "    \n",
    "    if columns is None:\n",
    "        columns = df.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "\n",
    "    for column in columns:\n",
    "        if column in df.columns:\n",
    "            # Check if the column contains NaN values\n",
    "            if df[column].isnull().any():\n",
    "                # Get the mode (most frequent value)\n",
    "                mode = df[column].mode()[0]\n",
    "                # Fill NaN values with the mode\n",
    "                df[column] = df[column].fillna(mode)\n",
    "            \n",
    "            # Convert the column to categorical type (if not already)\n",
    "            if not pd.api.types.is_categorical_dtype(df[column]):\n",
    "                df[column] = df[column].astype(\"category\")\n",
    "                \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T23:12:42.318812Z",
     "iopub.status.busy": "2025-02-23T23:12:42.318514Z",
     "iopub.status.idle": "2025-02-23T23:12:42.339498Z",
     "shell.execute_reply": "2025-02-23T23:12:42.338419Z",
     "shell.execute_reply.started": "2025-02-23T23:12:42.318785Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def find_matches(df, column, string_to_match, min_ratio=90, limit=10):\n",
    "    \"\"\"\n",
    "    Finds approximate matches of a given string in a specified column.\n",
    "    Returns a list of matched strings that meet or exceed the similarity threshold.\n",
    "    \"\"\"\n",
    "    unique_values = df[column].dropna().unique()\n",
    "    matches = process.extract(string_to_match, unique_values, limit=limit, scorer=fuzz.token_sort_ratio)\n",
    "    close_matches = [match[0] for match in matches if match[1] >= min_ratio]\n",
    "    return close_matches\n",
    "\n",
    "def replace_matches(df, column, string_to_match, min_ratio=90, limit=10):\n",
    "    \"\"\"\n",
    "    Replaces values in a column based on approximate string matches exceeding a given similarity ratio.\n",
    "    \"\"\"\n",
    "    close_matches = find_matches(df, column, string_to_match, min_ratio, limit)\n",
    "    df[column] = df[column].replace(close_matches, string_to_match)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T23:12:42.341018Z",
     "iopub.status.busy": "2025-02-23T23:12:42.340720Z",
     "iopub.status.idle": "2025-02-23T23:12:42.359406Z",
     "shell.execute_reply": "2025-02-23T23:12:42.358221Z",
     "shell.execute_reply.started": "2025-02-23T23:12:42.340992Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def handle_high_cardinality(df, threshold=50):\n",
    "    high_card_cols = []\n",
    "    for col in df.select_dtypes(include=[\"object\", \"category\"]).columns:\n",
    "        if df[col].nunique() > threshold:\n",
    "            high_card_cols.append(col)\n",
    "            # Agrupar valores poco frecuentes en \"otros\"\n",
    "            freq = df[col].value_counts(normalize=True)\n",
    "            df[col] = df[col].apply(lambda x: x if freq[x] > 0.01 else \"otros\")\n",
    "    return df, high_card_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T23:12:42.360968Z",
     "iopub.status.busy": "2025-02-23T23:12:42.360604Z",
     "iopub.status.idle": "2025-02-23T23:12:42.383182Z",
     "shell.execute_reply": "2025-02-23T23:12:42.382036Z",
     "shell.execute_reply.started": "2025-02-23T23:12:42.360939Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def clean_column_values(df, column, value_type='categorical', string_to_match=None, min_ratio=90, limit=10):\n",
    "    \"\"\"General function to clean values in a column depending on the value_type (categorical, numeric, etc.).\"\"\"\n",
    "    if value_type == 'categorical':\n",
    "        # Check and fix categorical inconsistencies\n",
    "        print(f\"Cleaning categorical values in {column}...\")\n",
    "        cat_counts = categoric_incosistent_wrang(df, column)\n",
    "        # Optional: Add logic to clean values based on fuzzy matching here\n",
    "        # Example: Replace similar values using find_matches and replace_matches\n",
    "        if string_to_match:\n",
    "            df = replace_matches(df, column, string_to_match, min_ratio, limit)\n",
    "        \n",
    "    elif value_type == 'numeric' and string_to_match:\n",
    "        # Perform string matching and replacements if needed\n",
    "        df = replace_matches(df, column, string_to_match, min_ratio, limit)\n",
    "        \n",
    "    elif value_type == 'date':\n",
    "        # Add date-related cleaning logic (if needed)\n",
    "        pass\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T23:12:42.384627Z",
     "iopub.status.busy": "2025-02-23T23:12:42.384303Z",
     "iopub.status.idle": "2025-02-23T23:12:42.405760Z",
     "shell.execute_reply": "2025-02-23T23:12:42.404522Z",
     "shell.execute_reply.started": "2025-02-23T23:12:42.384601Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_distribution(df, kind=\"hist\", bins=30, figsize=(15, 10)):\n",
    "    \"\"\"\n",
    "    Visualizes the distribution of all numeric columns in subplots.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame.\n",
    "    - kind: Type of plot (\"hist\" for histogram, \"kde\" for density plot).\n",
    "    - bins: Number of bins for the histogram (only applies if kind=\"hist\").\n",
    "    - figsize: Size of the figure.\n",
    "    \"\"\"\n",
    "    # Select only numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    if not numeric_cols.any():\n",
    "        print(\"No numeric columns to plot.\")\n",
    "        return\n",
    "    \n",
    "    # Calculate the number of rows and columns for subplots\n",
    "    n_cols = 3  # Number of subplot columns per row\n",
    "    n_rows = (len(numeric_cols) // n_cols) + 1\n",
    "    \n",
    "    # Create the figure and subplots\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n",
    "    axes = axes.flatten()  # Flatten the axes array for easier access\n",
    "    \n",
    "    # Plot each numeric column\n",
    "    for i, col in enumerate(numeric_cols):\n",
    "        if kind == \"hist\":\n",
    "            sns.histplot(df[col], bins=bins, kde=True, ax=axes[i])\n",
    "        elif kind == \"kde\":\n",
    "            sns.kdeplot(df[col], ax=axes[i])\n",
    "        axes[i].set_title(f\"Distribution of {col}\")\n",
    "        axes[i].set_xlabel(\"\")\n",
    "        axes[i].set_ylabel(\"\")\n",
    "    \n",
    "    # Hide empty axes if there are more subplots than columns\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T23:12:42.406919Z",
     "iopub.status.busy": "2025-02-23T23:12:42.406644Z",
     "iopub.status.idle": "2025-02-23T23:12:42.427033Z",
     "shell.execute_reply": "2025-02-23T23:12:42.425954Z",
     "shell.execute_reply.started": "2025-02-23T23:12:42.406896Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def detect_outliers(df, threshold=1.5, plot=True):\n",
    "    \"\"\"\n",
    "    Detects outliers in all numeric columns using the IQR method and optionally plots them.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame.\n",
    "    - threshold: Threshold for outlier calculation (default is 1.5).\n",
    "    - plot: If True, generates a boxplot highlighting the outliers.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with the detected outliers.\n",
    "    \"\"\"\n",
    "    # Select only numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    if not numeric_cols.any():\n",
    "        print(\"No numeric columns to analyze.\")\n",
    "        return None\n",
    "    \n",
    "    outliers_list = []\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - threshold * IQR\n",
    "        upper_bound = Q3 + threshold * IQR\n",
    "        \n",
    "        # Filter outliers\n",
    "        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "        if not outliers.empty:\n",
    "            outliers[\"outlier_column\"] = col  # Add a column to identify the source of the outlier\n",
    "            outliers_list.append(outliers)\n",
    "    \n",
    "    if outliers_list:\n",
    "        outliers_df = pd.concat(outliers_list).drop_duplicates()\n",
    "        \n",
    "        # Plot outliers if requested\n",
    "        if plot:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.boxplot(data=df[numeric_cols], orient=\"h\", palette=\"Set2\")\n",
    "            plt.title(\"Boxplot of Numeric Columns with Outliers Highlighted\")\n",
    "            plt.xlabel(\"Value\")\n",
    "            plt.ylabel(\"Column\")\n",
    "            \n",
    "            # Highlight outliers\n",
    "            for col in numeric_cols:\n",
    "                col_outliers = outliers_df[outliers_df[\"outlier_column\"] == col]\n",
    "                if not col_outliers.empty:\n",
    "                    plt.scatter(col_outliers[col], [col] * len(col_outliers), color=\"red\", label=\"Outliers\")\n",
    "            \n",
    "            # Avoid duplicate labels in the legend\n",
    "            handles, labels = plt.gca().get_legend_handles_labels()\n",
    "            by_label = dict(zip(labels, handles))\n",
    "            plt.legend(by_label.values(), by_label.keys())\n",
    "            \n",
    "            plt.show()\n",
    "        \n",
    "        return outliers_df\n",
    "    else:\n",
    "        print(\"No outliers detected.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T23:12:42.428584Z",
     "iopub.status.busy": "2025-02-23T23:12:42.428136Z",
     "iopub.status.idle": "2025-02-23T23:12:42.450530Z",
     "shell.execute_reply": "2025-02-23T23:12:42.449432Z",
     "shell.execute_reply.started": "2025-02-23T23:12:42.428553Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_correlation_matrix(df):\n",
    "    corr = df.corr()\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(corr, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "    plt.title(\"Correlation Matrix\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T23:12:42.451994Z",
     "iopub.status.busy": "2025-02-23T23:12:42.451589Z",
     "iopub.status.idle": "2025-02-23T23:12:42.471307Z",
     "shell.execute_reply": "2025-02-23T23:12:42.469937Z",
     "shell.execute_reply.started": "2025-02-23T23:12:42.451954Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def clean_text_column(df, column):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    df[column] = df[column].apply(lambda x: re.sub(r\"[^\\w\\s]\", \"\", str(x).lower()))\n",
    "    df[column] = df[column].apply(lambda x: \" \".join([word for word in x.split() if word not in stop_words]))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T23:12:42.472780Z",
     "iopub.status.busy": "2025-02-23T23:12:42.472400Z",
     "iopub.status.idle": "2025-02-23T23:12:42.487980Z",
     "shell.execute_reply": "2025-02-23T23:12:42.486904Z",
     "shell.execute_reply.started": "2025-02-23T23:12:42.472744Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def export_clean_data(df, path, format=\"csv\"):\n",
    "    if format == \"csv\":\n",
    "        df.to_csv(path, index=False)\n",
    "    elif format == \"excel\":\n",
    "        df.to_excel(path, index=False)\n",
    "    print(f\"Data exported successfully to {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opcional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T23:12:42.489660Z",
     "iopub.status.busy": "2025-02-23T23:12:42.489254Z",
     "iopub.status.idle": "2025-02-23T23:12:42.506108Z",
     "shell.execute_reply": "2025-02-23T23:12:42.504843Z",
     "shell.execute_reply.started": "2025-02-23T23:12:42.489618Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def categoric_incosistent_wrang(df, column):\n",
    "    \"\"\"Returns a sorted list of unique values in a column, first alphabetically, \n",
    "    then by the frequency of occurrences.\"\"\"\n",
    "    cat_counts = df[column].value_counts().to_dict()\n",
    "    sorted_values = sorted(cat_counts.items(), key=lambda x: (x[0], x[1]))\n",
    "    return sorted_values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T23:12:42.507631Z",
     "iopub.status.busy": "2025-02-23T23:12:42.507294Z",
     "iopub.status.idle": "2025-02-23T23:12:42.527581Z",
     "shell.execute_reply": "2025-02-23T23:12:42.526362Z",
     "shell.execute_reply.started": "2025-02-23T23:12:42.507602Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def handle_outliers(df, outliers, action=\"remove\", **kwargs):\n",
    "    \"\"\"\n",
    "    Handles outliers in a DataFrame based on the specified action.\n",
    "\n",
    "    Parameters:\n",
    "    - df: Original DataFrame.\n",
    "    - outliers: DataFrame with detected outliers (from detect_outliers).\n",
    "    - action: Action to perform. Options:\n",
    "        - \"remove\": Remove outliers from the DataFrame (default).\n",
    "        - \"impute\": Impute outliers with median, mean, or a custom value.\n",
    "        - \"transform\": Transform outliers using log scaling or winsorization.\n",
    "        - \"flag\": Add a column indicating if a row is an outlier.\n",
    "        - \"segment\": Split the DataFrame into two: one with outliers and one without.\n",
    "    - **kwargs: Additional arguments depending on the action:\n",
    "        - For \"impute\":\n",
    "            - method: Imputation method (\"median\", \"mean\", or a custom value).\n",
    "        - For \"transform\":\n",
    "            - method: Transformation method (\"log\" for log scaling, \"winsorize\" for winsorization).\n",
    "            - limits: Limits for winsorization (default [0.05, 0.05]).\n",
    "\n",
    "    Returns:\n",
    "    - Depending on the action, returns the modified DataFrame, a segmented DataFrame, or None.\n",
    "    \"\"\"\n",
    "    if outliers is None or outliers.empty:\n",
    "        print(\"No outliers to handle.\")\n",
    "        return df\n",
    "\n",
    "    if action == \"remove\":\n",
    "        print(\"Removing outliers...\")\n",
    "        df_cleaned = df[~df.index.isin(outliers.index)]\n",
    "        return df_cleaned\n",
    "\n",
    "    elif action == \"impute\":\n",
    "        # Default imputation method is median\n",
    "        method = kwargs.get(\"method\", \"median\")\n",
    "        print(f\"Imputing outliers using {method}...\")\n",
    "        \n",
    "        for col in outliers[\"outlier_column\"].unique():\n",
    "            if method == \"median\":\n",
    "                value = df[col].median()\n",
    "            elif method == \"mean\":\n",
    "                value = df[col].mean()\n",
    "            else:\n",
    "                value = kwargs.get(\"value\")  # Custom value\n",
    "                if value is None:\n",
    "                    raise ValueError(\"You must provide a value for custom imputation.\")\n",
    "            \n",
    "            df.loc[outliers.index, col] = value\n",
    "        return df\n",
    "\n",
    "    elif action == \"transform\":\n",
    "        # Default transformation method is winsorization\n",
    "        method = kwargs.get(\"method\", \"winsorize\")\n",
    "        print(f\"Transforming outliers using {method}...\")\n",
    "        \n",
    "        for col in outliers[\"outlier_column\"].unique():\n",
    "            if method == \"log\":\n",
    "                df[col] = np.log1p(df[col])  # Log scaling\n",
    "            elif method == \"winsorize\":\n",
    "                limits = kwargs.get(\"limits\", [0.05, 0.05])  # Default winsorization limits\n",
    "                df[col] = winsorize(df[col], limits=limits)\n",
    "            else:\n",
    "                raise ValueError(\"Invalid transformation method. Use 'log' or 'winsorize'.\")\n",
    "        return df\n",
    "\n",
    "    elif action == \"flag\":\n",
    "        print(\"Adding outlier flag column...\")\n",
    "        df[\"is_outlier\"] = df.index.isin(outliers.index)\n",
    "        return df\n",
    "\n",
    "    elif action == \"segment\":\n",
    "        print(\"Splitting DataFrame into with and without outliers...\")\n",
    "        df_outliers = df[df.index.isin(outliers.index)]\n",
    "        df_no_outliers = df[~df.index.isin(outliers.index)]\n",
    "        return df_outliers, df_no_outliers\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid action. Use 'remove', 'impute', 'transform', 'flag', or 'segment'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T23:12:42.529163Z",
     "iopub.status.busy": "2025-02-23T23:12:42.528713Z",
     "iopub.status.idle": "2025-02-23T23:12:42.551076Z",
     "shell.execute_reply": "2025-02-23T23:12:42.550166Z",
     "shell.execute_reply.started": "2025-02-23T23:12:42.529121Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
